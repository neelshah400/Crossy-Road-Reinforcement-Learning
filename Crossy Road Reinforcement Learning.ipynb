{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Crossy Road](https://www.crossyroad.com/) is an arcade video game built around the age-old joke \"Why did the chicken cross the road?\" In the game, the chicken (controlled by the player) has to cross the road without getting hit by vehicles.\n",
    "\n",
    "We want to develop a reinforcement learning (RL) game agent capable of playing Crossy Road. The gameâ€™s endless and random nature makes it a great candidate for RL.\n",
    "\n",
    "The agent will learn to maximize its score by getting the chicken to cross the road and avoid obstacles in its path, with the ultimate goal of crossing the road as many times as possible without collisions. Once the agent is capable of successfully getting the chicken to cross the road and reach the goal position, another goal could be to minimize the time it takes for the chicken to cross the road.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing the [Python](https://www.python.org/) libraries necessary for this project and configuring some things.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelshah/Documents/Projects/Crossy-Road-Reinforcement-Learning/.venv/lib/python3.11/site-packages/gymnasium/utils/play.py:29: UserWarning: \u001b[33mWARN: matplotlib is not installed, run `pip install gymnasium[other]`\u001b[0m\n",
      "  logger.warn(\"matplotlib is not installed, run `pip install gymnasium[other]`\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossy Road Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to implement a Crossy Road environment, which will encapsulate our representation of the reinforcement learning problem that the game poses.\n",
    "\n",
    "For this, we will utilize the [Gymnasium](https://gymnasium.farama.org/) library (a fork of the [OpenAI Gym](https://openai.com/research/openai-gym-beta) library), which provides a standard API for RL and various reference environments.\n",
    "\n",
    "Specifically, we will use the [Freeway](https://gymnasium.farama.org/environments/atari/freeway/) environment, which models an [Atari](https://atari.com/) game that closely resembles Crossy Road. This gives us a Pythonic interface to work with, which we can later use to develop RL models and create an agent that can play Crossy Road successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by initializing the environment using Gymnasium.\n",
    "\n",
    "We pass in the following arguments (documented [here](https://gymnasium.farama.org/environments/atari/)) to specify the environment:\n",
    "\n",
    "**Environment Flavor:**\n",
    "\n",
    "The environment `id`, `mode`, and `difficulty` combine to specify the specific flavor of the environment:\n",
    "\n",
    "- `id=\"ALE/Freeway-v5\"`: simulates the Atari game Freeway via the [Arcade Learning Environment (ALE)](https://github.com/Farama-Foundation/Arcade-Learning-Environment) through the [Stella](https://stella-emu.github.io/) emulator\n",
    "- `mode=0`: selects [Game 1 (Lake Shore Drive, Chicago, 3 A.M.)](https://atariage.com/manual_html_page.php?SoftwareLabelID=192) as the map to use\n",
    "- `difficulty=0`: selects the default difficulty setting\n",
    "\n",
    "**Stochasticity:**\n",
    "\n",
    "As stated in the documentation:\n",
    "\n",
    "> As the Atari games are entirely deterministic, agents can achieve state-of-the-art performance by simply memorizing an optimal sequence of actions while completely ignoring observations from the environment.\n",
    "\n",
    "To combat this, we use `frameskip` and `repeat_action_probability`:\n",
    "\n",
    "- `frameskip=4`: enables frame skipping (sets the number of frames to skip on each skip to $4$)\n",
    "- `repeat_action_probability=0.25`: enables sticky actions (sets the probability of repeating the previous action instead of executing the current action to $25\\%$)\n",
    "\n",
    "**Simulation:**\n",
    "\n",
    "The parameters `full_action_space` and `render_mode` are used to specify how the environment is simulated:\n",
    "\n",
    "- `full_action_space=False`: limits the action space to the $3$ legal actions we will actually use instead of all $18$ possible actions that can be performed on an Atari 2600 console\n",
    "- `render_mode=\"human\"`: specifies that the game should be rendered in human mode, displaying the screen and enabling game sounds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    id=\"ALE/Freeway-v5\",\n",
    "    mode=0,\n",
    "    difficulty=0,\n",
    "    obs_type=\"rgb\",\n",
    "    frameskip=4,\n",
    "    repeat_action_probability=0.25,\n",
    "    full_action_space=False,\n",
    "    render_mode=\"rgb_array\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to learn a little more about how out environment is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the observation space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (210, 160, 3), uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This observation space represents the RGB image that is displayed to a human player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's move on to the action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action space represents the actions that the chicken can take in each step:\n",
    "\n",
    "| Value | Meaning |\n",
    "| :---: | :-----: |\n",
    "| $0$   | `NOOP`  |\n",
    "| $1$   | `UP`    |\n",
    "| $2$   | `DOWN`  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's move on to the reward range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-inf, inf)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the reward range is $(-\\infty, \\infty)$.\n",
    "\n",
    "We don't really know the specifics, but more information can be found in the [game manual](https://atariage.com/manual_html_page.php?SoftwareLabelID=192)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-Environment Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "email": "neelshah@terpmail.umd.edu",
    "name": "Neel Shah"
   },
   {
    "email": "aabagri@terpmail.umd.edu",
    "name": "Anish Bagri"
   },
   {
    "email": "nsankar@terpmail.umd.edu",
    "name": "Nathan Sankar"
   },
   {
    "email": "agokaram@terpmail.umd.edu",
    "name": "Akul Gokaram"
   },
   {
    "email": "asharda@terpmail.umd.edu",
    "name": "Aditya Sharda"
   }
  ],
  "date": "May 7, 2024",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "title": "Crossy Road Reinforcement Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
